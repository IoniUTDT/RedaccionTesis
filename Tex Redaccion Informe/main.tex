\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{sidecap}
\usepackage{listings}             % Include the listings-package

\title{Informe de tesis de licenciatura}
\author{Ionatan Perez}
\date{\today}

\begin{document}


\lstset{language=Pascal}          % Set your language (you can change the language for each code-block optionally)

\maketitle

\clearpage 

\tableofcontents  %lo ponemos para trabajo largos que necesiten indice
\clearpage

\section{Introducción}

    Los mecanismos y dispositivos de sustitución sensorial (SSD) son técnicas en las cuales los estimulo generados en nuestro entorno y que normalmente son percibido a través de un dado sentido son reemplazados por otro estimulo (pero que codifica información equivalente) de manera que pueda ser percibido a través de un sentido sensorial alternativo. Típicamente estas técnicas son utilizadas por personas que carecen de alguno de los sentidos para poder percibir lo que de otra manera no podrían. Ejemplos cotidianos de estas técnicas son por ejemplo el bastón que suelen usar las personas ciegas, las marcas y guías en baldosas, el código braille, los timbres que encienden luces, las escalas cromáticas aptas para daltónicos, etc. 
    
    En particular, dentro de los problemas biológicos de percepción, el problema de la ceguera es un problema que conlleva limitaciones muy importantes para las personas que la sufren y que afecta a cerca de diez millones de personas en el mundo \cite{NroCiegos}. Como paliativo a dicha situación existen diferentes técnicas (algunas de muy baja complejidad, pero funcionalidad limitada) para sustituir sensorialmente los estímulos, como puede ser un bastón que al tacto permita reconocer obstáculos. Otras técnicas, mucho mas complejas y en actual desarrollo, apuntan a restituir la capacidad de visión mediante implantes que estimulen artificialmente al sistema nervioso\cite{Implantes1,Implantes2}, pero actualmente se encuentran en un estado muy temprano de desarrollo, solo sirven en casos específicos, poseen poca resolución y son muy invasivas \cite{Implantes3,Implantes4}. 
    
    Alternativamente a las técnicas que buscan restituir la visión en personas ciegas, están las técnicas que sustituyen los estímulos, estos estímulos pueden ser reemplazados por estímulos táctiles o bien auditivos. La representación táctil de los estímulos visuales comparte la propiedad intrínseca de ser percibida en un sistema de coordenadas bidimencional (la piel y la retina ambos poseen sensores distribuidos en una superficie). Los trabajos pioneros en este técnica fueron realizados por Paul Bach-y-Rita \cite{Tactil1} quien diseñó un dispositivo que sujeto en la espalda de las personas podía reproducir imágenes filmadas por una cámara. Sin embargo la baja resolución de este dispositivo (20x20 pixeles), junto a su tamaño hicieron que sea poco practico. En posteriores trabajos \cite{Tactil2} se reemplazó el dispositivo por uno colocado en la legua (que posee una sensibilidad muy alta) donde además se reemplazó el mecanismo de estímulo por uno de pequeñas descargas eléctricas. Este trabajo solucionó en buena parte el problema de la portabilidad del dispositivo (que media 3cmx3cm con 12x12 pixeles), pero sin embargo su capacidad de distinguir objetos sigue siendo reducida \cite{Tactil3}, comparable (sin entrenamiento) a 20/860 \footnote{Esta manera de indicar la capacidad visual de una persona representa cuan lejos tiene que estar (medido en pies si el numerador es 20) de dos puntos para distinguirlos en relación a una persona con visión plena. Se suele considerar a una persona con problemas de ceguera cuando su capacidad esta por debajo de 20/200}. Estudios \cite{Tactil4} muestran que con entrenamiento los sujetos pueden mejorar en cu capacidad de percepción, sin embargo hay una limitación básica en la resolución que permite este dispositivos que esta dado por la separación espacial de los electrodos y la capacidad de la lengua de distinguirlos. 
    
    \begin{figure}
        \center
        \includegraphics[width=0.8\textwidth]{Imagenes/Voice2.png}
        \caption{Dispositivo de sustitución sensorial vOICe y ejemplo de representación de las imagenes a través de dicho dispositivo.}
        \label{fig:Voice2}
    \end{figure}
    
    \begin{figure}
        \center
        \includegraphics[width=0.8\textwidth]{Imagenes/ImagenVoice1.png}
        \caption{Perdida de resolución al utilizar una tecnología tipo vOICe, se compara la imagen original con la imagen pixelada luego del procesamiento y la resolución real observada en sujetos. \url{http://dx.doi.org/10.1371/journal.pone.0033136}}
        \label{fig:Voice1}
    \end{figure}
    
    \begin{figure}
        \center
        \includegraphics[width=0.7\textwidth]{Imagenes/Voice3.png}
        \caption{Ejemplo del sonido correspondiente a imágenes pertenecientes a diferentes categorías y la capacidad del distinguirlas en sujetos que fueron entrenados durante periodos de tiempo prolongado en la tecnología vOICe. \url{http://dx.doi.org/10.1016/j.neuron.2012.08.026}}
        \label{fig:Voice3}
    \end{figure}
    
    
    El otro mecanismo por el cual se puede realizar sustitución sensorial es transformando el estimulo visual en un estímulo sonoro. El primer prototipo de esta tecnología (denominada vOICe) fue desarrollado en 1992 \cite{Voice1} como un dispositivo portátil, de bajo costo y de resolución mayor a las alcanzadas por los métodos de sustitución táctil (ver figura \ref{fig:Voice1}). Con el tiempo se fueron desarrollando versiones mejores, de mayor resolución y adaptadas a las tecnologías mas modernas (ver figura \ref{fig:Voice2}).
    
    La tecnología vOICe (\url{https://www.seeingwithsound.com/}), a diferencia de la sustitución sensorial táctil presenta la libertad de elegir que parámetros del sonido se eligen para representar los parámetros que usualmente se observan con la vista. A priori hay una amplia libertad en la representación si utilizan efectos sonoros complejos, sin embargo, recurriendo a las características básica del sonido hay tres grados de libertad a la hora de crear un sonido: el tono o frecuencia, el volumen y la duración. Estas tres características son las que utiliza el vOICe para representar la información visual (codificada en escala de grises). El vOICe toma una imagen, la pixela, y representa la coordenada vertical de cada pixel en la frecuencia, la coordenada horizontal en el tiempo (o duración del pixel) y la intensidad (o brillo) del pixel en el volumen o intensidad del sonido. En otras palabras lo que hace es interpretar cada columna de la imagen como una antitransformada de fourier del sonido a generar. 
    
    A partir de esta concepción de sustitución sensorial, el vOICe se fue desarrollando sobre diferentes plataformas tecnológicas siendo hoy una aplicación disponible en diversas plataformas, incluyendo PCs, celulares, y otros dispositivos que sean capaces de grabar imágenes y reproducir sonidos. Además cuenta con herramientas integradas de utilidad para personas ciegas como ser comandos por vos, filtros del la imagen a procesar por color, geolocalizacion, etc. La version para celulares pude consultarse en \url{https://play.google.com/store/apps/details?id=vOICe.vOICe} y un manual para realizar el entrenamiento adecuado para su uso en \url {https://www.seeingwithsound.com/manual/The_vOICe_Training_Manual.htm} 
    
    En paralelo al desarrollo de la tecnología vOICe como aplicación, se realizaron otras pruebas e investigaciones, algunas sobre variantes en la representación sonora, como ser el caso de la denominada PSVA \cite{VoiceVariante1} que difiere del vOICe en que la escala vertical esta discretizada no en función de la resolución como numero de pixeles, sino respetando saltos que difieran en notas fijas, además posee una región de mayor resolución en la zona central de la imagen, simulando el funcionamiento del ojo. Otra variante es el denominado SmartSight que filtra y agrupa secciones de la imagen antes de realizar una representación sonora en forma de notas musicales \cite{VoiceVariantes2, VoiceVariantes3}. También sobre la base de codificar diferentes aspectos en términos de propiedades sonoras mas complejas, se esta desarrollando una versión similar el vOICe denominada EyeMusic que codifica en términos de notas la altura de los pixeles, pero utilizando diferentes instrumentos para representar diferentes colores \cite{VoiceVariantes4}.
    
    Por otro lado a partir del imágenes y mediciones de actividad cerebral y de entrenamiento se han realizado estudios acerca del funcionamiento subyacente al mecanismo de sustitución sensorial. Hay estudios que muestran que en el caso de personas ciegas de nacimiento se activas zonas típicas de procesamiento de la información para el canal sensorial utilizado, mientras que en personas que perdieron la visión siendo adultos se activan zonas del procesamiento de imágenes (frente a un mismo estimulo), o que la modulación de la actividad cerebral difiere entre personas que perdieron la vista a edad temprana y personas que simplemente reciben el estimulo con la vista tapada \cite{VoiceSubyacente1,VoiceSubyacente3}. También hay estudios que muestran que con entrenamiento los sujetos aprenden a reconocer patrones fijos e incluso pueden transferir el aprendizaje entre figuras similares \cite{VoiceSubyacente2}. Por otro lado estudios sobre la tecnología vOICe muestran que los sujetos son capaces de entrenar e integrar la percepción en tareas de reconocimiento de geometría espacial, siendo capaces de ubicar y señalar posicionamiento de objetos en un entorno 3D donde pueden cambiar el punto de vista de la cámara con la que observan los estímulos \cite{VoiceSubyacente4}
    
    Los trabajos previos sobre la tecnología vOICe muestran que los sujetos entrenados (que pueden ser tanto ciegos como videntes\cite{VoiceEntrenamiento1}) pueden aprender a distinguir entre categorías imágenes complejas \cite{VoiceEntrenamiento2} (como los que se observan en la figura \ref{fig:Voice3}), nosotros en nuestro trabajo intentamos sobre la base de un sistema de representación equivalente estudiar la capacidad no de interpretar figuras complejas, sino sencillas, pero en términos de categorías geométricas. 
    
    La idea detrás de la propuesta fue que, si bien a la hora de interactuar con entornos complejos es importante que el sistema de sustitución sensorial permita representar de forma equivalente información compleja, muchas actividades no requieren interactuar con estímulos complejos. Muchas veces los estímulos o bien son intrínsecamente sencillos (por ejemplo los iconos y contornos en la navegación dentro de una computadora) o bien se puede preprocesar la imagen para extraer con algoritmos las información mas relevante antes de realizar la sustitución sensorial. Por otro lado, estudiar la capacidad de detectar conceptos geométricos abstractos podría permitir inspeccionar, dentro de la compleja tarea de reconocer patrones, saber que aspectos son mas difíciles de identificar, cuales mas fáciles, y de que depende la dificultad a la hora de interpretar los sonidos. También teníamos como objetivo tratar de identificar en que procesos del aprendizaje se podía observar efectos de transferencia (tanto dentro de una categoría como en orientaciones y otras trasformaciones de simetría)
    
    Con ese objetivo en mente fue que nos propusimos realizar experimentos en los cuales se presentara a sujetos videntes estímulos sonoros codificados según la lógica del vOICe en pos de observar cual era la capacidad de los sujetos de distinguir los aspectos geométricos en estos estímulos. 


\section{Desarrollo Metodológico}

\subsection{Panorama general del trabajo realizado}

    Poder realizar los experimentos propuestos conllevo una larga lista de desafíos, elecciones y problemas técnicos a resolver, en muchos casos interrelacionados y desconociendo la dificultad que representaría cada elección a priori. Cualquiera de los diseños experimentales que nos propusiéramos tenía que consistir en la presentación de estímulos sonoros al usuario (o sujeto experimental) que debían ser respondido con alguna elección. Posteriormente correlacionando los estímulos presentados con las respuestas se podría evaluar la capacidad de percepción del sujeto. Pero la elección de estímulos, la secuencia en la cual presentarlos, el modo en que interactuaría el sujeto con el experimento, las condiciones de entorno en que sería realizado, y las preguntas especificas que buscábamos responder eran en un principio cuestiones abiertas.
    
    Algunas cuestiones que si estaban definidas eran que en un principio todos los sujetos serian videntes porque experimentos previos de entrenamiento \cite{VoiceEntrenamiento3} indicaban que los videntes presentan capacidades similares de aprender la representación que las personas ciegas, y conseguir sujetos tanto como interactuar con ellos durante el procedimiento experimental es muchísimo más fácil si los sujetos son videntes. Eventualmente una vez obtenidos resultados se podrían contrastar y validar en personas no videntes. 
    
    Por otro lado el experimento requeriría algún tipo de software que mostrara los estímulos y registrara las respuestas. Ese software tenía que ser desarrollado, y para evitar limitaciones a la hora de tomar futuras decisiones elegimos desarrollar la aplicación y toda la programación en LibGDX que es un framework de java que permite compilar a java para ejecutar el experimento en un entorno controlado de laboratorio; a java para android para realizar experimentos menos controlados pero más masivos en celulares; o a javascript para HTML5 con la idea de realizar experimentos a distancia. 
    La elección de este entorno de programación que daba mucha flexibilidad a la hora de diseñar el procedimiento experimental tenía como contrapartida ciertas limitaciones a la hora de usar librerías y paquetes prearmados lo que implicó finalmente un mayor trabajo de adaptación y programación para realizar tareas que en un entorno más especifico quizás ya estaban resueltas. 
    
    Los desafíos que tuvimos que encarar a lo largo de todo el trabajo de la tesis probablemente puedan englobarse en unas pocas categorías, sin embargo la interacción y avance en cada una de ellas no fue lineal ni independiente por lo que su raconto a lo largo de este informe no va a respetar un orden estrictamente temporal sino más bien conceptual. Agrupando en categorías las dificultad que tuvimos que ir superando se pueden resumir en:
    
    \begin{itemize}
        \item Generar los estímulos. Diseñar la creación en forma sistemática y paramétrica de conjuntos de estímulos que permitieran probar las hipótesis de trabajo. 
        \item Adaptar, caracterizar y testear un algoritmo compatible con el resto del código que transformara los estímulos en su correspondiente representación sonora, y que respondiera a la lógica del vOICe. 
        \item Diseñar los experimentos que quisiéramos realizas y en función de ello generar los estímulos y niveles (instrucciones para crear un conjuntos de trials (o pruebas) consecutivos con las especificaciones necesarias).
        \item Diseñar la lógica de ejecución en tiempo real mediante la cual el experimento adapta los estímulos a mostrar en función de las respuestas de los usuarios (esto fue particularmente importante en los últimos experimentos)
        \item Diseñar una aplicación que fuera capaz de interpretar e integrar en una interfaz con el usuario toda la información relacionada a los estímulos y niveles creados manejando en tiempo real la elección de estímulos y parámetros según correspondiera, así como el correcto registro de múltiples indicadores que permitieran luego reconstruir por completo lo realizado por el sujeto experimental. 
        \item Construir un servidor que permitiera almacenar los datos en forma remota y que sirviera para realizar luego consultas a la hora de procesar la información en busca de resultados.
        \item Procesar los datos registrados para validar las hipótesis del diseño experimental y proponer adaptaciones o cambios en al mismo. 
    \end{itemize}
    
    En términos temporales el primer desafío que nos propusimos fue hacer un experimento en el que pudiéramos saber si los usuarios eran capaces de distinguir figuras geométricas, para ello pensamos en segmentos paralelos o no paralelos, ángulos rectos o no rectos y cuadriláteros cuadrados o no cuadrados (esto últimos son una combinación de las dos categorías anteriores y permitirían evaluar procesos de transferencia). El resultado (luego de muchas pruebas preliminares, ajustes, calibraciones y horas de programación) fue satisfactorio en tanto los sujetos eran capaces de interpretar al menos en términos muy básicos las figuras geométricas (generadas medio al azar) y distinguir entre grandes categorías o ejemplos muy diferentes. Esto no era obvio que fuera a suceder ya que los trabajos previos requerían decenas de horas de entrenamiento mientras que nuestros experimentos tenían que involucrar mucho menos tiempo de entrenamiento. Sin embargo surgieron dos problemas a la hora de pensar en un proceso de entrenamiento o medición mas cuidadoso. En primer lugar resultó evidente que la representación sonora es fuertemente no invariante frente a rotaciones, es clave la orientación de los segmentos que conforman las figuras a la hora de establecer la dificultad de interpretarlos y esta dependencia no la teníamos caracterizada. Por otro lado no teníamos ningún parámetro de la dificultad para cada estímulo en particular, por lo tanto no teníamos manera de armar un conjunto de estímulos que de alguna manera pudieran distinguir la capacidad en la zona de parámetros donde no saturara la medición como muy fácil o muy difícil. 
    
    Cabe aclarar que todas las mediciones de esta primer etapa fueron realizadas en forma esporádica y sin llevar un protocolo unificado que permitiera extraer conclusiones o resultados numéricamente validos. Por esto, cuando ya contábamos con una versión de la aplicación bastante funcional y una noción de las dificultades encontradas decidimos realizar un primer experimento piloto en el cual intentaríamos medir la dependencia con la orientación en la dificultad de distinguir entre categorías. Este experimento implico definir y ajustar varias cuestiones relacionadas al diseño experimental y los algoritmos necesarios para implementarlo. El primero y mas importante fue que en lugar de distinguir entre categorías para estímulos aleatorios necesitábamos caracterizar la capacidad de los sujetos en una tarea especifica y para eso queríamos realizar un experimento tipo Quest donde midiéramos el umbral de detección en función de la orientación. Este cambio requería por un lado generar secuencias de estímulos similares que variaran en una única dimensión o parámetro de manera de poder fluctuar dicho parámetro hasta encontrar el punto donde el usuario pasara de distinguir el estimulo a no distinguirlo. Por otro lado requería adaptar la lógica de funcionamiento de la aplicación y de los niveles para que pudieran ajustar la dificultad del estimulo mostrado en tiempo real en lugar de repetir una secuencia preestablecida. Además en la realización de este experimento pusimos a prueba todas las dificultades inherentes a la logística de realizar un experimento en laboratorio siguiendo un protocolo formal.
    
    Los resultados de este experimento mostraron que efectivamente la dependencia de la dificultad con la orientación existía y era muy marcada, pero también mostraron que había una enorme variabilidad intersujeto por lo cual no tenia sentido establecer la dificultad de un estimulo como parámetro intrínseco del estímulo. Por otro lado en este experimento descartamos evaluar los cuadriláteros por carecer las características geométricas necesarias. 
    
    A esta altura la idea de medir y entrenar a los sujetos (como habíamos pensado en un principio) con estímulos genéricos y comunes a todos los sujetos resultaba inviable. También resultaba inviable la idea de hacer un experimento masivo (en celulares o internet) ya que para poder evaluar cualquier característica era necesario una calibración previa que demanda una disponibilidad de tiempo y continuidad difícil de conseguir en un experimento a distancia. Por ello ideamos un último experimento en condiciones controladas de laboratorio donde lo que buscaríamos observar era si el entrenamiento en alguna condición especifica producía una mejora en la performance del sujeto durante y luego del entrenamiento, y si se observa una transferencia de dicha mejora entre las condiciones evaluadas. Para eso diseñamos un protocolo en el cual medimos la performance inicial de los sujetos, luego a algunos sujetos los entrenamos (dándoles feedback acerca de sus respuestas) en la detección de ángulos, a otros en la detección de paralelas y a otros no los entrenamos. Por ultimo volvimos a medir la performance de todos los sujetos con la intención de poder comparar el efecto del entrenamiento. 
    
    Como parte de los cambios realizados en el protocolo, mejoramos el mecanismo de medición del umbral respecto al experimento anterior (cambiando la lógica de generación de estímulos, y el algoritmo dinámico para ajustar la dificultad porque el largo de los experimentos resultaba un limitante), elegimos un conjunto de configuraciones o niveles con un alto grado de simetría con el objetivo de distinguir si la posible transferencia dependía de algunos de estos invariantes, y seleccionamos las configuraciones donde los sujetos mostraban peor performance inicial con el objetivo de tener un mayor rango donde medir los efectos de la mejora. Como se puede observar en los resultados detallados más adelante los resultados de este experimento no dieron todo lo bien que esperábamos. La principal razón por la cual no pudimos observar resultados acordes a lo buscado la atribuimos al bajo numero de sujetos que conseguimos que participaran del experimento, lo cual invalida cualquier medición estadísticamente sólida y al hecho de que el proceso de entrenamiento parece ser mas bien de carácter cualitativo y no cualitativo, es decir que cuando los sujetos entienden lo que tienen que hacer presentan una mejora, pero que dicha mejora luego no se repite en el tiempo al menos en los tiempos y longitudes de entrenamiento realizado. 
    
\subsection{El proceso de creación de estímulos}
    
    Desarrollar el proceso de creación de estímulos implicó dos desafíos, por un lado disponer de alguna lógica, paramétrica de creación de figuras geométricas que sirvieran para testear nuestras hipótesis y por otro el de transformar los estímulos creados en términos visuales o geométricos a su representación sonora. Mientras que el primer aspecto requirió una constante adaptación y sofisticación en función de los cambios experimentales, el segundo fue resuelto al inicio de trabajo y la solución, con leves cambios sirvió para todas las pruebas y experimentos realizados. 
    
    La lógica del vOICe para transformar imágenes en sonidos es muy sencilla y genérica, de ahí su potencialidad para representar cualquier tipo de figura. Para la transformación lo que hace es partir la imagen una una cuadricula de NxM donde cada elemento se corresponde con un pixel en la imagen y a partir de esta matriz se genera el sonido a reproducir. Para ello, se considera el índice vertical (lo llamaremos i) de la matriz como el índice que recorre en frecuencias que frecuencia se debe reproducir y el índice horizontal (llamado j) indica en que tiempo de debe reproducir dicho elemento. En términos matemáticos es hacer una antitransformada de fourier usando cada columna como el espectro que corresponde a tiempos sucesivos. Esta operación (para sucesivas imágenes identificadas con el índice k) se puede calcular como el resultado de la ecuación \ref{ec:VoiceOriginal} donde T representa el tiempo total que dura cada imagen. Este algoritmo se puede interpretar visualmente en la figura \ref{fig:VoiceOriginal}
    
    \begin{figure}
        \center
        \includegraphics[width=0.5\textwidth]{Imagenes/VoiceOriginal.png}
        \caption{Ejemplo de representación sonora para el dispositivo original del vOICe \cite{Voice1} correspondiente a la ecuación \ref{ec:VoiceOriginal}.}
        \label{fig:VoiceOriginal}
    \end{figure}
    
    \begin{equation}
        \label{ec:VoiceOriginal}
        s(t) = \sum_{i,j,k}^{M,N,K} p_{i,j}^k \cdot sin(w_i \cdot t + \phi^k) \cdot \int_{k \cdot T}^{(k+1) \cdot T} \delta_x dx \cdot \int_{k \cdot T+j \cdot T/N}^{k \cdot T + (j+1) \cdot T/N} \delta_x dx
    \end{equation}

    Con este procesamiento en mente construimos un primer algoritmo que realizara dicha transformación y probamos escuchar como resultaba la representación sonora para imágenes de prueba realizadas con cualquier editor de gráficos en formato no vectorial. Nos encontramos con problemas inmediatos. Lo primero observado fue que si bien el sistema de transformación a priori no posee una limitación de resolución obvia, cuando la resolución es alta aparecen problemas inherentes a los saltos en la discretización de la imagen. La discretización en la coordenada horizontal hace que en realidad lo que parece un espectro puro para un pixel dado adquiera armónicos producto de que el pixel comienza y termina en un lapso corto de tiempo. Por lo tanto cuanto mayor sea la resolución mas seguido aparecen saltos de intensidad que se corresponden con la inclusión de chasquidos en el sonidos que distorsionan el sonido deseado. 
    
    Por otro lado la discretización vertical de la imagen hace que al utilizar resoluciones altas se creen secuencias de sonido de frecuencia muy similar que generan efectos de batido. Nuevamente, al incrementar la resolución utilizada estos problemas escalan rápidamente. 
    
    Estos dos problemas, que en un sistema de representación de imágenes complejas pueden pasar más desapercibidos o generar menos alteraciones, en nuestro caso representaban un impedimento fundamental para utilizar el algoritmo original de vOICe, porque no solo nos impedirían utilizar una representación de alta resolución sino que también incluirían en el sistema de representación un elemento indeseado que presenta una muy fuerte ruptura de simetría. Como nuestro objetivo estaba centrado precisamente en determinar la sensibilidad de los sujetos a aspectos geométricos y desconocíamos el rango de sensibilidad que esperábamos medir, basar todos los experimentos en una representación que a priori presenta una limitación tan evidente no tenia sentido. 
    
    Tuvimos por lo tanto que reformular el algoritmo de transformación de imágenes a sonido de manera de evitar estos inconvenientes. Necesitábamos un algoritmo que respetara la representación donde a mayor altura mayor frecuencia, pero que no tuviera los problemas inherente a la discretización en pixeles de las imágenes usuales. En otras palabras necesitábamos un sistema que preservara la información conceptual de los segmentos con los que queríamos construir geometría sin pasar por la instancia de representación en formato pixelado, y un algoritmo para transformar dicha información tanto a una imagen visual como a una representación sonora. Esto representaba dos nuevos desafíos, por un lado generar u manipular imágenes en formato vectorial (que es un formato donde se guarda la información independientemente de su representación) y por otro poder transformar esta información vectorial en los sonidos correspondientes. 
    
    Para la primera de las dos tareas decidimos crear y utilizar imágenes en formato SVG. El formato SVG es un estándar de representación (\url{https://www.w3.org/TR/SVG/}) para construir imágenes a partir de información vectorial con alto nivel de difusión y fácil codificación. Tiene la ventaja de que la mayoría de los navegadores y sistemas operativos modernos incluyen renderizadores capaces de mostrar como imagen este tipo de archivos, de que (al igual que los demás formatos vectoriales) permite almacenar información de una imagen arbitrariamente grande y compleja en pocas lineas de texto, de ser un estándar para el cual existen librerías en la mayoría de los lenguajes de programación capaces de manipularlo, y de ser un formato fácilmente legible incluso por humanos mirando el código fuente. En nuestro caso un factor importante a la hora de utilizar este formato fue que la elección era compatible con generar posteriores procedimientos experimentales que pudieran ser validados en contextos y dispositivos diferentes, pero conservando la información fundamental y conceptual de los estímulos utilizados. Un ejemplo de como se genera y visualiza la información en este formato puede verse en la figura \ref{fig:SVGtoPNG} donde muestra la representación para una de las figuras utilizadas en las pruebas preliminares. 
    
    El código para generar estos archivos fue variando según la etapa de desarrollo experimental de cada experimento de manera de ser cada vez más paramétrico y automatizado. El detalle de los parámetros utilizados en los últimos experimentos se comenta mas adelante, pero para las pruebas preliminares se generaron largas secuencias de figuras incluidas en las categorías de cuadriláteros, paralelas y ángulos donde se generaban fluctuaciones aleatorias sobre los ángulos formados, las separaciones y tamaños de las lineas, y las ubicaciones en el lienzo. Luego con estas imágenes ya creadas se seleccionaba para su utilización algunas que se adecuaran a los test a realizar.
    
    Algo que se implemento también en paralelo con la generación de los archivos SVG en si mismo, fue la creación de un archivo por imagen con un registro de las características de la figura generada en formato Json para que el software supiera interpretar el contenido de cada figura durante su ejecución. Junto a cada imagen se guardó las categorías geométricas a las que pertenecía dicha imagen (esto es fundamental para que el software pudiera reconocer si las respuestas indicadas por el usuario eran correctas o equivocadas), sus propiedades y parámetros geométricos con los que se la construyó (esta información queda registrada en los logs de las respuestas para que después se pueda corresponder el comportamiento del usuario en función de los valores con que se parametrizó la figura), y además se incluyo campos descriptivos a los efectos de facilitar la revisión del correcto funcionamiento del código y la detección de eventuales errores durante la fase de testeo. 
    
    \begin{figure}
        \center
        \includegraphics[width=\textwidth]{Imagenes/678SVG.png}
        \caption{Ejemplo de representación en código SVG de una figura de las utilizadas. Se observa que se trata de una variante de código XML. En el comentario inicial se almacena información acerca del contexto del algoritmo que lo crea. Luego se establece el estándar SVG y el tamaño del lienzo. Dentro, se pinta el fondo, y sobre él se agregan las lineas indicadas con las coordenadas de sus extremos. A la derecha se puede observar una renderización visual del contenido.}
        \label{fig:SVGtoPNG}
    \end{figure}
    
    Tambien, dado que finalmente el lenguaje utilizado para la programación de la aplicación no soportaba el uso de archivos SVG en tiempo real por ser algo poco eficiente en termino de uso de recursos (especialmente pensando en plataformas android) incluimos en el proceso de creación de los archivos SVG la creación de las correspondientes imagenes y su posterior agrupamiento (por niveles) en archivos ATLAS optimizados para el uso de memoria y procesador en tiempo de ejecución.
    
    En cuanto a la tarea de construir un algoritmo que transformara las figuras geométricas en sonido, lo primero que hicimos para limitar el problema fue restringirnos a segmentos rectos que se debían corresponder con rampas en frecuencia.
    
    Transformar el archivo SVG en un archivo de audio requirió solucionar las siguientes etapas:
    
    \begin{itemize}
        \item Extraer del archivo SVG la informacion de cada linea a crear
        \item Transformar los parametros correspondientes a extremos geometricos de cada linea en parametros de tiempo y frecuencia para los extremos de la rampa de sonido.
        \item Crear la rampa de sonido correspondiente a cada segmento
        \item Componer y normalizar correctamente el conjunto de rampas para conformar una secuencia de bits que represente la información sonora completa correspondiente a la imagen
        \item Transformar la secuencia de bits en un archivo MP3 utilizable por el resto del software
    \end{itemize}
    
    Para la extracción de la información correspondiente a cada linea, utilizamos una libreria existente en Java que permite recorrer la información de archivos XML, una vez extraida la información de los extremos de cada segmento y el tamaño del lienzo, debimos implementar los siguientes chequeos y procesamientos:
    
    \begin{itemize}
        \item Revisar que la información este ordenada correctamente, es decir que primero este el extremo izquierdo y luego el derecho, en caso de que no, alternarlos. 
        \item Revisar que los extremos de los segmentos se encontraran dentro del lienzo, ya que esto no tiene porque suceder si la imagen fue creada mal (a veces algunas combinaciones de parámetros mal diseñados pueden dar que un segmento exceda exceda el tamaño del lienzo). En este caso de detectar que el segmento excede el lienzo el código además de advertir en modo debug con un warning recorta el segmento hasta que toca el extremo del lienzo.
        \item Realizar un cambio de escala según correspondiera a la configuración de como interpretar los tamaños. Esta funcionalidad nunca la utilizamos, pero al diseñar en términos conceptuales la representación sonora existía la libertad de establecer una escala de tamaño para el lienzo o para el pixel, en otras palabras no es lo mismo interpretar que un lienzo de 100x100 (tamaño usado) representa todo el espacio porque cada pixel representa un centésimo del espacio a interpretar que cualquier imagen sin importar su tamaño en pixeles debe abarcar todo el espacio disponible. 
    \end{itemize}
    
    Una vez obtenida las coordenadas de los extremos del segmento a transformar en rampa sonora debíamos encontrar los correspondiente valores de tiempo y frecuencia para cada extremo. Para los valores temporales la cuenta se sencilla pues es lineal, por lo que la transformación esta dada por la ecuación \ref{ec:xTot}. Para los valores de frecuencia la cuenta debe incluir la elección de la escala que en el caso de las frecuencias no tiene porque ser lineal, pues el oído no interpreta en una escala lineal las frecuencias. Dado que el oído interpreta en escala logarítmica decidimos respetar esta escala incluyendo en el código la opción de utilizar una escala lineal que en los hechos nunca usamos. 
    
    También con la idea de utilizar la mayor parte del espectro audible en un principio decidimos utilizar el rango de los 100 a los 8000 Hz, sin embargo este rango resultó un tanto excesivo en el uso de los agudos creando en sonidos molestos a la hora de realizar los experimentos, por lo que luego de utilizar este rango en buena parte de las pruebas piloto decidimos hacer ingeniería inversa sobre la tecnología vOICe midiendo el espectro de frecuencias que dicha tecnología utilizaba. Encontramos que como limite superior de frecuencia el vOICe utilizaba 4000 Hz, un valor que efectivamente resulto mucho mas razonable. El limite inferior no lo pudimos detectar al realizar el espectro por lo que mantuvimos el valor de 100 Hz utilizado originalmente. 
    
    Por otro lado al implementar el código, e involucrar operaciones exponenciales nos encontramos con el problema de que numéricamente las computadoras no manejan fácilmente valores tan grandes (por mas que el resultado final fuera un numero manejable), por eso tuvimos que realizar las operaciones para calcular los parámetros de la representación utilizando una escala simbólica que fuera de 0 a 1 y luego reescalamos los resultados obtenidos a la escala real utilizada. Realizando las cuentas se puede observar que partiendo de la relación \ref{ec:yToFrecInicial} se llega a las expresiones  \ref{ec:yToF1}, \ref{ec:yToF2} y \ref{ec:yToF3}. En la implementación numérica de estas soluciones siempre utilizamos como base b a la base decimal. 
    
    \begin{equation}
        \label{ec:xTot}
        t = x \cdot \frac{T}{NroPixels}
    \end{equation}
    
    \begin{equation}
        \label{ec:yToFrecInicial}
        Frec(y) = A \cdot b^{y} + B
    \end{equation}
    
    con 
    
    \begin{equation*}
        Frec(Y_{max}) = A \cdot b^{y_{max}} + B
    \end{equation*}
    
    \begin{equation*}
        Frec(Y_{min}) = A \cdot b^{y_{min}} + B
    \end{equation*}
    
    donde se asume que $y_{min}=0$ por lo que que reescalando con Y* = Y/$Y_{max}$ queda que
    
    \begin{equation*}
        Frec(y*) = A \cdot b^{y*} + B
    \end{equation*}
    
    \begin{equation*}
        Frec(Y*_{max}) = A \cdot b^{1} + B
    \end{equation*}
    
    \begin{equation*}
        Frec(Y*_{min}) = A \cdot b^{0} + B
    \end{equation*}
    
    de donde sale que 
    
    \begin{equation}
        \label{ec:yToF1}
        A = \frac{Frec(Y_{max}) - Frec(Y_{min}) }{b^1-b^0}
    \end{equation}
    
    \begin{equation}
        \label{ec:yToF2}
        B = ((Frec(Y_{max}) + Frec(Y_{min})) - A \cdot (b^1 + b^0)) / 2
    \end{equation}
    
    \begin{equation}
        \label{ec:yToF3}
        Frec(Y) = A \cdot b^{Y/Y_{max}} + B
    \end{equation}
\clearpage


\begin{thebibliography}{9}

\bibitem{NroCiegos}
  WHO (2011) Fact Sheet Nu282
\bibitem{Implantes1}
Dowling J (2008) Current and future prospects for optoelectronic retinal prostheses. Eye 23: 1999–2005.
\bibitem{Implantes2}
Weiland JD, Cho AK, Humayun MS (2011) Retinal prostheses: current clinical results and future needs. Ophthalmology 118: 2227–2237.
\bibitem{Implantes3}
E Striem-Amit, A Bubic, A Amedi - 2012: Neurophysiological Mechanisms Underlying Plastic Changes and Rehabilitation following Sensory Loss in Blindness and Deafness
\bibitem{Implantes4}
Zrenner E, Bartz-Schmidt KU, Benav H, Besch D, Bruckmann A, et al. (2010) Subretinal electronic chips allow blind patients to read letters and combine them to words. Proceedings of the Royal Society B: Biological Sciences 278: 1489–1497.
\bibitem{Tactil1}
Bach-Y-Rita P, Collins C.C, Saunders F.A, White B, Scadden L. Vision substitution by tactile image projection. Nature. 1969;221:963–964. 
\bibitem{Tactil2}
Bach-Y-Rita P, Kaczmarek K.A, Tyler M.E, Garcia-Lara J. Form perception with a 49-point electrotactile stimulus array on the tongue: A technical note. J Rehabil Res Dev. 1998;35:427–430.
\bibitem{Tactil3}
Sampaio E, Maris S, Bach-Y-Rita P. Brain plasticity: ‘Visual’ acuity of blind persons via the tongue. Brain Res. 2001;908:204–207. 
\bibitem{Tactil4}
Chebat D.R, Rainville C, Kupers R, Ptito M. Tactile-‘visual’ acuity of the tongue in early blind individuals. Neuroreport. 2007;18:1901–1904
\bibitem{Voice1}
Meijer P.B. An experimental system for auditory image representations. IEEE Trans Biomed Eng. 1992;39:112–121.
\bibitem{VoiceVariante1}
Capelle C, Trullemans C, Arno P, Veraart C. A real-time experimental prototype for enhancement of vision rehabilitation using auditory substitution. IEEE Trans Biomed Eng. 1998;45:1279–1293. 
\bibitem{VoiceVariantes2}
Cronly-Dillon J, Persaud K, Gregory R.P. The perception of visual images encoded in musical form: A study in cross-modality information transfer. Proc Biol Sci. 1999;266:2427–2433. 
\bibitem{VoiceVariantes3}
Cronly-Dillon J, Persaud K.C, Blore R. Blind subjects construct conscious mental images of visual scenes encoded in musical form. Proc Biol Sci. 2000;267:2231–2238.
\bibitem{VoiceVariantes4}
Abboud, Sami, et al. "EyeMusic: Introducing a “visual” colorful experience for the blind using auditory sensory substitution." Restorative neurology and neuroscience 32.2 (2014): 247-257.
\bibitem{VoiceSubyacente1}
Poirier, Colline, Anne G. De Volder, and Christian Scheiber. "What neuroimaging tells us about sensory substitution." Neuroscience and Biobehavioral Reviews 31.7 (2007): 1064-1070.
\bibitem{VoiceSubyacente2}
Arno P, Capelle C, Wanet-Defalque M.C, Catalan-Ahumada M, Veraart C. Auditory coding of visual patterns for the blind. Perception. 1999;28:1013–1029. [PubMed]
\bibitem{VoiceSubyacente3}
Arno P, De Volder A.G, Vanlierde A. et al. Occipital activation by pattern recognition in the early blind using auditory substitution for vision. Neuroimage. 2001;13:632–645.
\bibitem{VoiceSubyacente4}
Auvray M, Hanneton S, O’Regan J.K. Learning to perceive with a visuo-auditory substitution system: Localisation and object recognition with ‘The vOICe’ Perception. 2007;36:416–430.
\bibitem{VoiceEntrenamiento1}
Striem-Amit, Ella, Miriam Guendelman, and Amir Amedi. "‘Visual’acuity of the congenitally blind using visual-to-auditory sensory substitution." PloS one 7.3 (2012): e33136.
\bibitem{VoiceEntrenamiento2}
Striem-Amit, Ella, et al. "Reading with sounds: sensory substitution selectively activates the visual word form area in the blind." Neuron 76.3 (2012): 640-652.
\bibitem{VoiceEntrenamiento3}
Arno, Patricia, et al. "Auditory substitution of vision: pattern recognition by the blind." Applied Cognitive Psychology 15.5 (2001): 509-519.

\end{thebibliography}

\end{document}